#!/usr/bin/env python3
"""
Pixabay Music Downloader Tool
T·∫£i nh·∫°c MP3 t·ª´ Pixabay v·ªõi kh·∫£ nƒÉng ch·ªçn range
"""

import requests
from bs4 import BeautifulSoup
import re
import os
import urllib.parse
from urllib.parse import urljoin
import time
import json
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from threading import Lock

class PixabayMusicDownloader:
    def __init__(self):
        self.session = requests.Session()
        # Headers m·∫°nh h∆°n ƒë·ªÉ gi·∫£ l·∫≠p browser th·∫≠t
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        self.music_list = []
        # Threading locks for thread-safe operations
        self.print_lock = Lock()
        self.progress_lock = Lock()
        
    def parse_pixabay_page(self, url: str) -> List[Dict]:
        """
        Parse trang Pixabay ƒë·ªÉ l·∫•y danh s√°ch nh·∫°c
        """
        print(f"üîç ƒêang t·∫£i trang: {url}")
        
        try:
            # Th·ª≠ v·ªõi delay v√† timeout ƒë·ªÉ tr√°nh b·ªã block
            time.sleep(2)
            response = self.session.get(url, timeout=30, allow_redirects=True)
            
            print(f"üìä Status code: {response.status_code}")
            print(f"üìä Content length: {len(response.content):,} bytes")
            
            if response.status_code == 403:
                print("‚ö†Ô∏è  403 Forbidden - Th·ª≠ ph∆∞∆°ng ph√°p kh√°c...")
                return self._try_alternative_methods(url)
            
            response.raise_for_status()
            
            music_items = self._parse_response_content(response.content, url)
            self.music_list = music_items
            return music_items
            
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i trang: {e}")
            print("üí° Th·ª≠ ph∆∞∆°ng ph√°p thay th·∫ø...")
            return self._try_alternative_methods(url)
    
    def _parse_single_page(self, page_url: str, page_num: int) -> Dict:
        """
        Parse m·ªôt trang ƒë∆°n l·∫ª - d√πng cho threading
        Returns: Dict v·ªõi th√¥ng tin k·∫øt qu·∫£ parse
        """
        result = {
            'page_num': page_num,
            'success': False,
            'items': [],
            'error': None,
            'url': page_url
        }
        
        try:
            with self.print_lock:
                print(f"üìÑ [{threading.current_thread().name}] ƒêang crawl trang {page_num}: {page_url}")
            
            # Parse trang hi·ªán t·∫°i
            page_items = self.parse_pixabay_page(page_url)
            
            if page_items:
                # Th√™m page number v√†o t·ª´ng item
                for item in page_items:
                    item['page'] = page_num
                
                result['items'] = page_items
                result['success'] = True
                
                with self.print_lock:
                    print(f"‚úÖ [{threading.current_thread().name}] Trang {page_num}: Th√™m {len(page_items)} tracks")
            else:
                result['error'] = "Kh√¥ng t√¨m th·∫•y tracks"
                with self.print_lock:
                    print(f"‚ùå [{threading.current_thread().name}] Trang {page_num}: Kh√¥ng t√¨m th·∫•y tracks")
                    
        except Exception as e:
            result['error'] = str(e)
            with self.print_lock:
                print(f"‚ùå [{threading.current_thread().name}] L·ªói khi crawl trang {page_num}: {e}")
        
        return result

    def parse_multiple_pages(self, base_url: str, start_page: int = 1, end_page: int = 3, max_workers: int = 3) -> List[Dict]:
        """
        Parse nhi·ªÅu trang Pixabay v·ªõi pagination t·ª´ start_page ƒë·∫øn end_page s·ª≠ d·ª•ng multi-threading
        max_workers: S·ªë thread t·ªëi ƒëa cho parsing (m·∫∑c ƒë·ªãnh 3 ƒë·ªÉ kh√¥ng l√†m qu√° t·∫£i server)
        """
        all_music_items = []
        total_pages = end_page - start_page + 1
        
        print(f"üìö B·∫Øt ƒë·∫ßu crawl t·ª´ trang {start_page} ƒë·∫øn trang {end_page} ({total_pages} trang)...")
        print(f"üßµ S·ª≠ d·ª•ng {max_workers} threads song song cho parsing")
        print("=" * 70)
        
        # Chu·∫©n b·ªã danh s√°ch parse jobs
        parse_jobs = []
        for page_num in range(start_page, end_page + 1):
            # T·∫°o URL cho t·ª´ng trang
            if page_num == 1 and 'pagi=' not in base_url:
                page_url = base_url
            else:
                # Th√™m parameter pagi cho trang ti·∫øp theo
                separator = '&' if '?' in base_url else '?'
                if 'pagi=' in base_url:
                    # Replace existing pagi parameter
                    page_url = re.sub(r'pagi=\d+', f'pagi={page_num}', base_url)
                else:
                    page_url = f"{base_url}{separator}pagi={page_num}"
            
            parse_jobs.append((page_url, page_num))
        
        print(f"üìã ƒê√£ chu·∫©n b·ªã {len(parse_jobs)} jobs parsing...")
        
        # Kh·ªüi t·∫°o counters
        successful_pages = 0
        failed_pages = 0
        completed_pages = 0
        
        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ parse song song
        with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="Parser") as executor:
            # Submit t·∫•t c·∫£ jobs
            future_to_job = {
                executor.submit(self._parse_single_page, page_url, page_num): (page_url, page_num) 
                for page_url, page_num in parse_jobs
            }
            
            print(f"üéØ ƒê√£ submit {len(future_to_job)} parse tasks...")
            print("‚è≥ ƒêang crawl... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)")
            print("-" * 70)
            
            # Thu th·∫≠p k·∫øt qu·∫£ theo th·ª© t·ª± ho√†n th√†nh
            page_results = {}
            
            for future in as_completed(future_to_job):
                page_url, page_num = future_to_job[future]
                completed_pages += 1
                
                try:
                    result = future.result()
                    page_results[page_num] = result
                    
                    with self.progress_lock:
                        if result['success']:
                            successful_pages += 1
                        else:
                            failed_pages += 1
                        
                        # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
                        progress = (completed_pages / len(parse_jobs)) * 100
                        print(f"\nüìä Ti·∫øn ƒë·ªô parsing: {completed_pages}/{len(parse_jobs)} ({progress:.1f}%)")
                        print(f"‚úÖ Th√†nh c√¥ng: {successful_pages} | ‚ùå Th·∫•t b·∫°i: {failed_pages}")
                        
                except Exception as e:
                    with self.print_lock:
                        print(f"‚ùå L·ªói unexpected khi x·ª≠ l√Ω trang {page_num}: {e}")
                    failed_pages += 1
        
        # S·∫Øp x·∫øp v√† gh√©p k·∫øt qu·∫£ theo th·ª© t·ª± trang
        print(f"\nüîó ƒêang gh√©p k·∫øt qu·∫£ t·ª´ {len(page_results)} trang...")
        
        current_index = 1
        for page_num in sorted(page_results.keys()):
            result = page_results[page_num]
            if result['success'] and result['items']:
                # Update index ƒë·ªÉ kh√¥ng tr√πng l·∫∑p
                for item in result['items']:
                    item['index'] = current_index
                    current_index += 1
                
                all_music_items.extend(result['items'])
                print(f"üìÑ Trang {page_num}: ƒê√£ th√™m {len(result['items'])} tracks")
        
        print(f"\nüìä T·ªîNG K·∫æT CRAWLING:")
        print(f"‚úÖ Th√†nh c√¥ng: {successful_pages}/{total_pages} trang")
        print(f"‚ùå Th·∫•t b·∫°i: {failed_pages}/{total_pages} trang")
        print(f"üìä T·ª∑ l·ªá th√†nh c√¥ng: {(successful_pages/total_pages*100):.1f}%")
        print(f"üìÑ Range: Trang {start_page}-{end_page}")
        print(f"üéµ T·ªïng tracks: {len(all_music_items)}")
        print("=" * 70)
        
        self.music_list = all_music_items
        return all_music_items
    
    def _try_alternative_methods(self, url: str) -> List[Dict]:
        """
        Th·ª≠ c√°c ph∆∞∆°ng ph√°p thay th·∫ø khi g·∫∑p l·ªói 403 ho·∫∑c blocked
        """
        print("üîÑ ƒêang th·ª≠ c√°c ph∆∞∆°ng ph√°p thay th·∫ø...")
        
        # Method 1: Th·ª≠ v·ªõi session m·ªõi v√† headers kh√°c
        try:
            print("üìã Ph∆∞∆°ng ph√°p 1: Session m·ªõi + headers kh√°c...")
            new_session = requests.Session()
            new_session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',
                'Accept': '*/*',
                'Accept-Language': 'en-US,en;q=0.5',
                'Referer': 'https://pixabay.com/',
                'Origin': 'https://pixabay.com'
            })
            
            time.sleep(3)
            response = new_session.get(url, timeout=30)
            if response.status_code == 200:
                print("‚úÖ Th√†nh c√¥ng v·ªõi ph∆∞∆°ng ph√°p 1!")
                return self._parse_response_content(response.content, url)
                
        except Exception as e:
            print(f"‚ùå Ph∆∞∆°ng ph√°p 1 th·∫•t b·∫°i: {e}")
        
        # Method 2: Th·ª≠ URL ƒë∆°n gi·∫£n h∆°n 
        try:
            print("üìã Ph∆∞∆°ng ph√°p 2: URL ƒë∆°n gi·∫£n...")
            simple_url = "https://pixabay.com/music/search/piano/"
            response = self.session.get(simple_url, timeout=30)
            if response.status_code == 200:
                print("‚úÖ Th√†nh c√¥ng v·ªõi URL ƒë∆°n gi·∫£n!")
                return self._parse_response_content(response.content, simple_url)
                
        except Exception as e:
            print(f"‚ùå Ph∆∞∆°ng ph√°p 2 th·∫•t b·∫°i: {e}")
        
        # Method 3: G·ª£i √Ω s·ª≠ d·ª•ng URL tr·ª±c ti·∫øp
        print("üìã Ph∆∞∆°ng ph√°p 3: H∆∞·ªõng d·∫´n l·∫•y URL tr·ª±c ti·∫øp...")
        print("""
üí° G·ª¢I √ù: Pixabay c√≥ th·ªÉ c·∫ßn truy c·∫≠p tr·ª±c ti·∫øp qua browser.
H√£y th·ª≠:
1. M·ªü {url} trong browser
2. M·ªü Developer Tools (F12)
3. T√¨m c√°c file .mp3 trong Network tab
4. Copy URL tr·ª±c ti·∫øp c·ªßa file MP3

Ho·∫∑c nh·∫≠p URL kh√°c ƒë·ªÉ th·ª≠:""".format(url=url))
        
        return self._create_demo_list()
    
    def _parse_response_content(self, content: bytes, url: str) -> List[Dict]:
        """
        Parse n·ªôi dung response th√†nh danh s√°ch nh·∫°c
        """
        soup = BeautifulSoup(content, 'html.parser')
        music_items = []
        
        # Debug: T√¨m hi·ªÉu c·∫•u tr√∫c HTML th·ª±c t·∫ø
        print("üîç ƒêang ph√¢n t√≠ch c·∫•u tr√∫c HTML...")
        
        # T√¨m c√°c patterns c·ª• th·ªÉ c·ªßa Pixabay (d·ª±a tr√™n HTML th·ª±c)
        patterns_to_try = [
            ('div[class*="audioRow"]', 'Pixabay audioRow containers'),
            ('div[class*="Row"]', 'Pixabay Row containers'),
            ('div[class*="item"]', 'div c√≥ class ch·ª©a "item"'),
            ('div[class*="media"]', 'div c√≥ class ch·ª©a "media"'),
            ('div[class*="result"]', 'div c√≥ class ch·ª©a "result"'),
            ('div[class*="track"]', 'div c√≥ class ch·ª©a "track"'),
            ('div[class*="audio"]', 'div c√≥ class ch·ª©a "audio"'),
            ('div[class*="music"]', 'div c√≥ class ch·ª©a "music"'),
            ('article', 'article elements'),
            ('div[data-id]', 'div c√≥ data-id'),
            ('.item', 'class item'),
            ('.media', 'class media'),
            ('.track', 'class track'),
            ('[data-track]', 'elements c√≥ data-track'),
            ('[data-audio]', 'elements c√≥ data-audio'),
        ]
        
        items = []
        for selector, description in patterns_to_try:
            try:
                found_items = soup.select(selector)
                if found_items:
                    print(f"‚úÖ T√¨m th·∫•y {len(found_items)} items v·ªõi selector: {description}")
                    items = found_items
                    break
                else:
                    print(f"‚ùå Kh√¥ng t√¨m th·∫•y v·ªõi selector: {description}")
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói v·ªõi selector {description}: {e}")
        
        if not items:
            # Fallback: t√¨m t·∫•t c·∫£ divs v√† filter
            print("üîÑ Th·ª≠ fallback method...")
            all_divs = soup.find_all('div')
            print(f"üìä T·ªïng s·ªë div tags: {len(all_divs)}")
            
            # T√¨m divs c√≥ th·ªÉ ch·ª©a th√¥ng tin nh·∫°c
            for div in all_divs[:50]:  # Ch·ªâ check 50 divs ƒë·∫ßu
                if any(keyword in str(div.get('class', [])).lower() for keyword in ['item', 'media', 'track', 'music', 'audio', 'result']):
                    items.append(div)
                elif any(attr.startswith('data-') for attr in div.attrs if 'id' in attr or 'track' in attr or 'audio' in attr):
                    items.append(div)
        
        print(f"üìã Cu·ªëi c√πng t√¨m th·∫•y {len(items)} items ƒë·ªÉ parse")
        
        for idx, item in enumerate(items):
            try:
                print(f"\nüîç ƒêang parse item {idx + 1}...")
                
                # Debug: In ra th√¥ng tin c∆° b·∫£n c·ªßa item
                item_classes = item.get('class', [])
                item_id = item.get('id', '')
                print(f"   Classes: {item_classes}")
                print(f"   ID: {item_id}")
                
                # T√¨m title theo c·∫•u tr√∫c Pixabay c·ª• th·ªÉ
                title = "Unknown Track"
                
                # Pixabay c√≥ class title--xxxxx trong structure
                title_selectors = [
                    'a[class*="title"]',  # a.title--7N7Nr
                    '[class*="title"]',   # C√°c element kh√°c c√≥ title
                    'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                    '[title]', '[alt]', 
                    '.nameAndTitle a:first-child',  # Link ƒë·∫ßu ti√™n trong nameAndTitle
                    'a[href*="/music/"]',  # Links ƒë·∫øn trang music
                    'span', 'div', 'p'
                ]
                
                for selector in title_selectors:
                    title_elem = item.select_one(selector)
                    if title_elem:
                        candidate_title = title_elem.get_text(strip=True) or title_elem.get('title', '') or title_elem.get('alt', '')
                        if candidate_title and len(candidate_title) > 2 and candidate_title != 'Unknown Track':
                            title = candidate_title
                            print(f"   ‚úÖ T√¨m th·∫•y title v·ªõi selector '{selector}': {title}")
                            break
                
                # T√¨m download link - Pixabay s·ª≠ d·ª•ng JavaScript cho download
                download_link = None
                
                # 1. T√¨m URL trang chi ti·∫øt (ƒë·ªÉ c√≥ th·ªÉ fetch sau)
                detail_link = None
                detail_links = item.find_all('a', href=True)
                for link in detail_links:
                    href = link.get('href', '')
                    if '/music/' in href and any(word in href for word in ['/', '-']):
                        detail_link = urljoin(url, href)
                        print(f"   ‚úÖ T√¨m th·∫•y detail page: {detail_link}")
                        break
                
                # 2. T√¨m audio elements (√≠t kh·∫£ nƒÉng c√≥)
                audio_elem = item.find('audio')
                if audio_elem and audio_elem.get('src'):
                    download_link = urljoin(url, audio_elem['src'])
                    print(f"   ‚úÖ T√¨m th·∫•y audio src: {download_link}")
                
                # 3. T√¨m data attributes c√≥ th·ªÉ ch·ª©a track ID
                track_id = None
                if not download_link:
                    for attr, value in item.attrs.items():
                        if 'data' in attr.lower() and ('id' in attr.lower() or 'track' in attr.lower()):
                            track_id = str(value)
                            print(f"   ‚úÖ T√¨m th·∫•y track ID: {track_id}")
                            break
                
                # 4. Extract ID t·ª´ detail link n·∫øu c√≥
                if not track_id and detail_link:
                    # Pixabay URLs th∆∞·ªùng c√≥ format: /music/title-123456/
                    id_match = re.search(r'-(\d+)/?$', detail_link)
                    if id_match:
                        track_id = id_match.group(1)
                        print(f"   ‚úÖ Extract track ID t·ª´ URL: {track_id}")
                
                # 5. ∆Øu ti√™n d√πng detail page ƒë·ªÉ fetch URL th·ª±c
                if detail_link:
                    download_link = detail_link
                    print(f"   üîó S·∫Ω fetch URL th·ª±c t·ª´ detail page: {detail_link}")
                elif track_id:
                    # Backup: th·ª≠ c√°c format kh·∫£ dƒ©
                    possible_formats = [
                        f"https://cdn.pixabay.com/audio/2023/{track_id}.mp3",
                        f"https://cdn.pixabay.com/audio/2024/{track_id}.mp3", 
                        f"https://pixabay.com/get/{track_id}.mp3",
                        f"https://pixabay.com/music/download/{track_id}.mp3"
                    ]
                    download_link = possible_formats[0]
                    print(f"   üîó T·∫°o download link gi·∫£ ƒë·ªãnh: {download_link}")
                
                # 6. T√¨m trong child elements n·∫øu v·∫´n ch∆∞a c√≥
                if not download_link:
                    for child in item.find_all(recursive=True):
                        for attr, value in child.attrs.items():
                            if any(ext in str(value).lower() for ext in ['.mp3', '.wav', '.m4a']) and 'http' in str(value):
                                download_link = urljoin(url, str(value))
                                print(f"   ‚úÖ T√¨m th·∫•y trong child: {download_link}")
                                break
                        if download_link:
                            break
                
                if download_link:
                    music_items.append({
                        'title': title,
                        'download_url': download_link,
                        'index': len(music_items) + 1
                    })
                    print(f"   ‚úÖ ƒê√£ th√™m v√†o danh s√°ch: {title}")
                else:
                    print(f"   ‚ùå Kh√¥ng t√¨m th·∫•y download link cho item n√†y")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói khi parse item {idx}: {e}")
                continue
        
        # N·∫øu kh√¥ng t√¨m th·∫•y g√¨, th·ª≠ t√¨m trong JavaScript/JSON data
        if not music_items:
            print("\nüîç T√¨m ki·∫øm trong JavaScript/JSON data...")
            scripts = soup.find_all('script')
            for idx, script in enumerate(scripts[:10]):  # Ch·ªâ check 10 scripts ƒë·∫ßu
                if script.string:
                    script_content = script.string
                    
                    # T√¨m URLs MP3 trong JavaScript
                    if any(keyword in script_content.lower() for keyword in ['mp3', 'audio', 'music', 'track']):
                        print(f"   üìú Script {idx + 1} c√≥ th·ªÉ ch·ª©a th√¥ng tin audio...")
                        
                        # Patterns ƒë·ªÉ extract URLs
                        url_patterns = [
                            r'["\']([^"\']*\.mp3[^"\']*)["\']',
                            r'["\']([^"\']*\.wav[^"\']*)["\']',
                            r'["\']([^"\']*\.m4a[^"\']*)["\']',
                            r'["\']([^"\']*audio[^"\']*\.mp3)["\']',
                            r'download["\']:\s*["\']([^"\']*)["\']',
                            r'src["\']:\s*["\']([^"\']*\.mp3[^"\']*)["\']'
                        ]
                        
                        for pattern in url_patterns:
                            urls = re.findall(pattern, script_content, re.IGNORECASE)
                            for url_match in urls:
                                if url_match and len(url_match) > 10:  # Filter out short false matches
                                    full_url = urljoin(url, url_match)
                                    music_items.append({
                                        'title': f"JS Track {len(music_items) + 1}",
                                        'download_url': full_url,
                                        'index': len(music_items) + 1
                                    })
                                    print(f"   ‚úÖ T√¨m th·∫•y URL trong JS: {url_match}")
                        
                        # T√¨m th√¥ng tin JSON
                        json_patterns = [
                            r'\{[^}]*"title"[^}]*"url"[^}]*\}',
                            r'\{[^}]*"name"[^}]*"src"[^}]*\}',
                            r'\{[^}]*"audio"[^}]*\}'
                        ]
                        
                        for pattern in json_patterns:
                            matches = re.findall(pattern, script_content, re.IGNORECASE)
                            for match in matches:
                                try:
                                    # Th·ª≠ parse JSON-like structure
                                    if '"title"' in match and '"url"' in match:
                                        title_match = re.search(r'"title":\s*"([^"]*)"', match)
                                        url_match = re.search(r'"url":\s*"([^"]*)"', match)
                                        if title_match and url_match:
                                            music_items.append({
                                                'title': title_match.group(1),
                                                'download_url': urljoin(url, url_match.group(1)),
                                                'index': len(music_items) + 1
                                            })
                                            print(f"   ‚úÖ T√¨m th·∫•y JSON track: {title_match.group(1)}")
                                except Exception as e:
                                    print(f"   ‚ö†Ô∏è  L·ªói parse JSON: {e}")
        
        print(f"\nüìä T·ªïng c·ªông t√¨m th·∫•y {len(music_items)} tracks")
        return music_items
    
    def _create_demo_list(self) -> List[Dict]:
        """
        T·∫°o danh s√°ch demo ƒë·ªÉ test tool
        """
        print("üéµ T·∫°o danh s√°ch demo ƒë·ªÉ test...")
        return [
            {
                'title': 'Demo Piano Track 1',
                'download_url': 'https://pixabay.com/music/download/demo1.mp3',
                'index': 1
            },
            {
                'title': 'Demo Piano Track 2', 
                'download_url': 'https://pixabay.com/music/download/demo2.mp3',
                'index': 2
            },
            {
                'title': 'Demo Piano Track 3',
                'download_url': 'https://pixabay.com/music/download/demo3.mp3', 
                'index': 3
            }
        ]
    
    def display_music_list(self):
        """
        Hi·ªÉn th·ªã danh s√°ch nh·∫°c v·ªõi s·ªë th·ª© t·ª±
        """
        if not self.music_list:
            print("üì≠ Kh√¥ng c√≥ nh·∫°c n√†o trong danh s√°ch")
            return
            
        print("\n" + "="*80)
        print("üéµ DANH S√ÅCH NH·∫†C T√åM TH·∫§Y")
        print("="*80)
        
        current_page = None
        for item in self.music_list:
            # Hi·ªÉn th·ªã header cho trang m·ªõi
            if 'page' in item and item['page'] != current_page:
                current_page = item['page']
                if current_page > 1:
                    print(f"\nüìÑ --- TRANG {current_page} ---")
            
            page_info = f" (Trang {item['page']})" if 'page' in item else ""
            print(f"{item['index']:3d}. {item['title']}{page_info}")
            print(f"     URL: {item['download_url'][:60]}...")
            print()
    
    def _try_get_real_download_url(self, fake_url: str, title: str) -> str:
        """
        Th·ª≠ l·∫•y URL download th·ª±c t·ª´ Pixabay
        """
        try:
            print(f"   üîç Th·ª≠ l·∫•y URL th·ª±c cho: {title}")
            
            # N·∫øu l√† detail page, th·ª≠ fetch v√† t√¨m download link
            if '/music/' in fake_url and not fake_url.endswith('.mp3'):
                print(f"   üìÑ Fetching detail page: {fake_url}")
                
                # Th√™m headers ƒë·ªÉ gi·∫£ l·∫≠p browser
                headers = {
                    'Referer': 'https://pixabay.com/',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                }
                
                response = self.session.get(fake_url, timeout=15, headers=headers)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    print(f"   üìä Detail page size: {len(response.content):,} bytes")
                    
                    # T√¨m trong JavaScript data
                    scripts = soup.find_all('script')
                    for script in scripts:
                        if script.string:
                            script_content = script.string
                            
                            # T√¨m URLs MP3 trong JavaScript v·ªõi patterns m·∫°nh h∆°n
                            patterns = [
                                r'"download"[^"]*"([^"]*\.mp3[^"]*)"',
                                r'"url"[^"]*"([^"]*\.mp3[^"]*)"',
                                r'"src"[^"]*"([^"]*\.mp3[^"]*)"',
                                r'cdn\.pixabay\.com/audio/[^"\']*\.mp3',
                                r'https://[^"\']*\.mp3',
                                r'["\']([^"\']*cdn\.pixabay\.com[^"\']*\.mp3)["\']'
                            ]
                            
                            for pattern in patterns:
                                matches = re.findall(pattern, script_content, re.IGNORECASE)
                                for match in matches:
                                    if match and len(match) > 20 and '.mp3' in match:
                                        if not match.startswith('http'):
                                            match = 'https:' + match if match.startswith('//') else 'https://' + match
                                        print(f"   ‚úÖ T√¨m th·∫•y URL trong JS: {match}")
                                        return match
                    
                    # T√¨m audio elements v√† download buttons
                    download_patterns = [
                        'audio[src]',
                        'source[src]',
                        'a[href*=".mp3"]',
                        '[data-url*=".mp3"]',
                        '[onclick*=".mp3"]'
                    ]
                    
                    for pattern in download_patterns:
                        elements = soup.select(pattern)
                        for elem in elements:
                            url_attrs = ['src', 'href', 'data-url', 'onclick']
                            for attr in url_attrs:
                                href = elem.get(attr, '')
                                if href and '.mp3' in href:
                                    if 'javascript:' not in href.lower():
                                        real_url = urljoin(fake_url, href)
                                        print(f"   ‚úÖ T√¨m th·∫•y URL t·ª´ {pattern}: {real_url}")
                                        return real_url
                    
                    print(f"   ‚ùå Kh√¥ng t√¨m th·∫•y URL download trong detail page")
            
            # N·∫øu l√† URL gi·∫£ ƒë·ªãnh, th·ª≠ test n√≥
            elif fake_url.endswith('.mp3'):
                print(f"   üß™ Test URL gi·∫£ ƒë·ªãnh: {fake_url}")
                try:
                    head_response = self.session.head(fake_url, timeout=8)
                    if head_response.status_code == 200:
                        content_type = head_response.headers.get('content-type', '')
                        if 'audio' in content_type.lower() or 'mpeg' in content_type.lower():
                            print(f"   ‚úÖ URL gi·∫£ ƒë·ªãnh ho·∫°t ƒë·ªông!")
                            return fake_url
                    print(f"   ‚ùå URL gi·∫£ ƒë·ªãnh kh√¥ng ho·∫°t ƒë·ªông: {head_response.status_code}")
                except:
                    print(f"   ‚ùå Kh√¥ng th·ªÉ test URL gi·∫£ ƒë·ªãnh")
                    
        except Exception as e:
            print(f"   ‚ö†Ô∏è  L·ªói khi l·∫•y URL th·ª±c: {e}")
        
        # Fallback: return detail page ƒë·ªÉ c√≥ th·ªÉ th·ª≠ manual
        print(f"   üîÑ Fallback: s·ª≠ d·ª•ng detail page")
        return fake_url

    def _get_next_file_index(self, download_folder: str) -> int:
        """
        Ki·ªÉm tra th∆∞ m·ª•c v√† tr·∫£ v·ªÅ s·ªë th·ª© t·ª± ti·∫øp theo ƒë·ªÉ tr√°nh ghi ƒë√® file c≈©
        """
        if not os.path.exists(download_folder):
            return 1
        
        max_index = 0
        try:
            files = os.listdir(download_folder)
            for filename in files:
                if filename.endswith('.mp3'):
                    # T√¨m pattern 001_, 002_, etc.
                    match = re.match(r'^(\d{3})_', filename)
                    if match:
                        index = int(match.group(1))
                        max_index = max(max_index, index)
            
            print(f"üìÇ T√¨m th·∫•y {len([f for f in files if f.endswith('.mp3')])} file MP3 trong th∆∞ m·ª•c")
            if max_index > 0:
                print(f"üìä S·ªë th·ª© t·ª± cao nh·∫•t hi·ªán t·∫°i: {max_index}")
                print(f"üÜï File m·ªõi s·∫Ω b·∫Øt ƒë·∫ßu t·ª´: {max_index + 1}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi scan th∆∞ m·ª•c: {e}")
        
        return max_index + 1

    def _download_single_file(self, item: Dict, download_folder: str, file_number: int) -> Dict:
        """
        Download m·ªôt file nh·∫°c ƒë∆°n l·∫ª - d√πng cho threading
        Returns: Dict v·ªõi th√¥ng tin k·∫øt qu·∫£ download
        """
        result = {
            'item': item,
            'success': False,
            'error': None,
            'filename': None,
            'file_size': 0
        }
        
        try:
            # L√†m s·∫°ch t√™n file
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', item['title'])
            filename = f"{file_number:03d}_{safe_title}.mp3"
            filepath = os.path.join(download_folder, filename)
            result['filename'] = filename
            
            with self.print_lock:
                print(f"‚¨áÔ∏è  [{threading.current_thread().name}] ƒêang download {item['index']}: {item['title']}")
            
            # Th·ª≠ l·∫•y URL th·ª±c tr∆∞·ªõc khi download
            real_url = self._try_get_real_download_url(item['download_url'], item['title'])
            
            # T·∫°o session ri√™ng cho thread n√†y ƒë·ªÉ tr√°nh xung ƒë·ªôt
            thread_session = requests.Session()
            thread_session.headers.update(self.session.headers)
            
            # Download file
            with self.print_lock:
                print(f"   üåê [{threading.current_thread().name}] Downloading t·ª´: {real_url}")
            
            response = thread_session.get(real_url, stream=True, timeout=30)
            response.raise_for_status()
            
            # Ki·ªÉm tra content type
            content_type = response.headers.get('content-type', '')
            if 'audio' not in content_type.lower() and 'mpeg' not in content_type.lower():
                with self.print_lock:
                    print(f"‚ö†Ô∏è  [{threading.current_thread().name}] C·∫£nh b√°o: File c√≥ th·ªÉ kh√¥ng ph·∫£i MP3 (Content-Type: {content_type})")
            
            # L∆∞u file
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            file_size = os.path.getsize(filepath)
            result['file_size'] = file_size
            
            if file_size > 1024 * 1024:  # > 1MB
                size_str = f"{file_size / (1024*1024):.1f}MB"
            else:
                size_str = f"{file_size:,} bytes"
            
            with self.print_lock:
                print(f"‚úÖ [{threading.current_thread().name}] Ho√†n th√†nh: {filename} ({size_str})")
            
            result['success'] = True
            
        except Exception as e:
            result['error'] = str(e)
            with self.print_lock:
                print(f"‚ùå [{threading.current_thread().name}] L·ªói download {item['title']}: {e}")
        
        return result

    def download_music_range(self, start_idx: int, end_idx: int, download_folder: str = "downloads", max_workers: int = 4):
        """
        Download nh·∫°c theo range t·ª´ start_idx ƒë·∫øn end_idx s·ª≠ d·ª•ng multi-threading
        max_workers: S·ªë thread t·ªëi ƒëa (m·∫∑c ƒë·ªãnh 4)
        """
        if not self.music_list:
            print("‚ùå Ch∆∞a c√≥ danh s√°ch nh·∫°c. Vui l√≤ng parse trang tr∆∞·ªõc.")
            return
        
        # Validate range
        if start_idx < 1 or end_idx > len(self.music_list) or start_idx > end_idx:
            print(f"‚ùå Range kh√¥ng h·ª£p l·ªá. Vui l√≤ng ch·ªçn t·ª´ 1 ƒë·∫øn {len(self.music_list)}")
            return
        
        # T·∫°o folder download
        os.makedirs(download_folder, exist_ok=True)
        
        # Ki·ªÉm tra th∆∞ m·ª•c v√† l·∫•y s·ªë th·ª© t·ª± ti·∫øp theo
        next_file_index = self._get_next_file_index(download_folder)
        
        # T√≠nh to√°n s·ªë file c·∫ßn download
        total_files = end_idx - start_idx + 1
        
        print(f"\nüöÄ B·∫Øt ƒë·∫ßu download t·ª´ {start_idx} ƒë·∫øn {end_idx} ({total_files} files)")
        print(f"üìÅ Th∆∞ m·ª•c l∆∞u: {download_folder}")
        print(f"üßµ S·ª≠ d·ª•ng {max_workers} threads song song")
        if next_file_index > 1:
            print(f"üî¢ S·ªë th·ª© t·ª± file s·∫Ω b·∫Øt ƒë·∫ßu t·ª´: {next_file_index}")
        print("-" * 60)
        
        # Chu·∫©n b·ªã danh s√°ch download jobs
        download_jobs = []
        for i in range(start_idx - 1, end_idx):
            if i >= len(self.music_list):
                break
            
            item = self.music_list[i]
            file_number = next_file_index + (i - (start_idx - 1))
            download_jobs.append((item, file_number))
        
        print(f"üìã ƒê√£ chu·∫©n b·ªã {len(download_jobs)} jobs download...")
        
        # Kh·ªüi t·∫°o counters
        success_count = 0
        failed_count = 0
        completed_count = 0
        
        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ download song song
        with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="Downloader") as executor:
            # Submit t·∫•t c·∫£ jobs
            future_to_job = {
                executor.submit(self._download_single_file, item, download_folder, file_number): (item, file_number) 
                for item, file_number in download_jobs
            }
            
            print(f"üéØ ƒê√£ submit {len(future_to_job)} download tasks...")
            print("‚è≥ ƒêang download... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)")
            print("-" * 60)
            
            # X·ª≠ l√Ω k·∫øt qu·∫£ khi c√°c thread ho√†n th√†nh
            for future in as_completed(future_to_job):
                item, file_number = future_to_job[future]
                completed_count += 1
                
                try:
                    result = future.result()
                    
                    with self.progress_lock:
                        if result['success']:
                            success_count += 1
                        else:
                            failed_count += 1
                        
                        # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
                        progress = (completed_count / len(download_jobs)) * 100
                        print(f"\nüìä Ti·∫øn ƒë·ªô: {completed_count}/{len(download_jobs)} ({progress:.1f}%)")
                        print(f"‚úÖ Th√†nh c√¥ng: {success_count} | ‚ùå Th·∫•t b·∫°i: {failed_count}")
                        
                except Exception as e:
                    with self.print_lock:
                        print(f"‚ùå L·ªói unexpected khi x·ª≠ l√Ω {item['title']}: {e}")
                    failed_count += 1
        
        print(f"\n" + "="*60)
        print(f"üèÅ HO√ÄN TH√ÄNH DOWNLOAD")
        print(f"üìä K·∫æT QU·∫¢ CU·ªêI C√ôNG:")
        print(f"   ‚úÖ Th√†nh c√¥ng: {success_count}/{len(download_jobs)}")
        print(f"   ‚ùå Th·∫•t b·∫°i: {failed_count}/{len(download_jobs)}")
        print(f"   üìä T·ª∑ l·ªá th√†nh c√¥ng: {(success_count/len(download_jobs)*100):.1f}%")
        print(f"üìÅ Th∆∞ m·ª•c: {os.path.abspath(download_folder)}")
        print("="*60)

def handle_direct_urls():
    """
    X·ª≠ l√Ω download t·ª´ URL tr·ª±c ti·∫øp
    """
    print("\nüì• DOWNLOAD TR·ª∞C TI·∫æP T·ª™ URL")
    print("=" * 50)
    print("Nh·∫≠p c√°c URL file MP3, m·ªói URL m·ªôt d√≤ng.")
    print("Nh·∫≠p 'done' ƒë·ªÉ ho√†n th√†nh:")
    
    urls = []
    while True:
        url = input(f"URL {len(urls) + 1}: ").strip()
        if url.lower() == 'done':
            break
        if url:
            urls.append(url)
    
    if not urls:
        print("‚ùå Kh√¥ng c√≥ URL n√†o ƒë∆∞·ª£c nh·∫≠p.")
        return
    
    # T·∫°o downloader v√† fake music list
    downloader = PixabayMusicDownloader()
    downloader.music_list = [
        {
            'title': f'Direct Download {i+1}',
            'download_url': url,
            'index': i + 1
        } for i, url in enumerate(urls)
    ]
    
    # Hi·ªÉn th·ªã v√† download
    downloader.display_music_list()
    
    try:
        print(f"\nüìù Download t·∫•t c·∫£ {len(urls)} file?")
        folder_input = input("Th∆∞ m·ª•c l∆∞u (Enter = 'downloads'): ").strip()
        folder = folder_input if folder_input else "downloads"
        
        confirm = input(f"\nX√°c nh·∫≠n download {len(urls)} file v√†o '{folder}'? (y/N): ").strip().lower()
        
        if confirm in ['y', 'yes']:
            downloader.download_music_range(1, len(urls), folder)
        else:
            print("‚ùå ƒê√£ h·ªßy download.")
            
    except KeyboardInterrupt:
        print("\n\n‚ùå ƒê√£ h·ªßy b·ªüi ng∆∞·ªùi d√πng.")
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")

def main():
    """
    H√†m main ƒë·ªÉ ch·∫°y tool
    """
    downloader = PixabayMusicDownloader()
    
    print("üéµ PIXABAY MUSIC DOWNLOADER")
    print("=" * 50)
    
    # URL m·∫∑c ƒë·ªãnh t·ª´ user
    default_url = "https://pixabay.com/vi/music/search/nh%e1%ba%a1c%20kh%c3%b4ng%20b%e1%ba%a3n%20quy%e1%bb%81n/?genre=piano+solo"
    
    # Nh·∫≠p URL (ho·∫∑c d√πng m·∫∑c ƒë·ªãnh)
    url_input = input(f"Nh·∫≠p URL Pixabay (Enter ƒë·ªÉ d√πng m·∫∑c ƒë·ªãnh):\n{default_url}\n> ").strip()
    url = url_input if url_input else default_url
    
    # H·ªèi v·ªÅ pagination
    print("\nüîÑ T√ôY CH·ªåN CRAWLING:")
    print("1. Ch·ªâ crawl trang ƒë·∫ßu ti√™n (nhanh)")
    print("2. Crawl nhi·ªÅu trang (ch·∫≠m h∆°n nh∆∞ng c√≥ nhi·ªÅu nh·∫°c h∆°n)")
    
    crawl_choice = input("Ch·ªçn (1/2, Enter = 1): ").strip()
    
    if crawl_choice == '2':
        try:
            start_page = int(input("T·ª´ trang (Enter = 1): ").strip() or "1")
            end_page = int(input("ƒê·∫øn trang (Enter = 3): ").strip() or "3")
            
            # Validate input
            start_page = max(start_page, 1)  # T·ªëi thi·ªÉu trang 1
            end_page = min(max(end_page, start_page), 100)  # T·ªëi ƒëa trang 100
            
            if start_page > end_page:
                start_page, end_page = end_page, start_page  # Swap n·∫øu ng∆∞·ª£c
            
            total_pages = end_page - start_page + 1
            
            # T√πy ch·ªçn threads cho parsing
            if total_pages > 1:
                parse_threads_input = input(f"S·ªë threads cho parsing (Enter = 3, t·ªëi ƒëa 5): ").strip()
                try:
                    parse_threads = int(parse_threads_input) if parse_threads_input else 3
                    parse_threads = min(max(parse_threads, 1), 5)  # Gi·ªõi h·∫°n t·ª´ 1-5
                except ValueError:
                    parse_threads = 3
                    print("‚ùå S·ªë kh√¥ng h·ª£p l·ªá, d√πng m·∫∑c ƒë·ªãnh 3 threads")
                
                print(f"üöÄ S·∫Ω crawl t·ª´ trang {start_page} ƒë·∫øn trang {end_page} ({total_pages} trang) v·ªõi {parse_threads} threads...")
                music_list = downloader.parse_multiple_pages(url, start_page, end_page, parse_threads)
            else:
                print(f"üöÄ S·∫Ω crawl t·ª´ trang {start_page} ƒë·∫øn trang {end_page} ({total_pages} trang)...")
                music_list = downloader.parse_multiple_pages(url, start_page, end_page, 1)
        except ValueError:
            print("‚ùå S·ªë kh√¥ng h·ª£p l·ªá, d√πng m·∫∑c ƒë·ªãnh trang 1-3")
            music_list = downloader.parse_multiple_pages(url, 1, 3, 3)
    else:
        print("üöÄ Crawl trang ƒë·∫ßu ti√™n...")
        music_list = downloader.parse_pixabay_page(url)
    
    if not music_list:
        print("‚ùå Kh√¥ng th·ªÉ l·∫•y danh s√°ch nh·∫°c t·ª´ trang n√†y.")
        
        # Cho ph√©p ng∆∞·ªùi d√πng th·ª≠ l·∫°i v·ªõi URL kh√°c
        while True:
            choice = input("""
üîÑ B·∫°n mu·ªën:
1. Th·ª≠ l·∫°i v·ªõi URL kh√°c
2. Nh·∫≠p tr·ª±c ti·∫øp URL file MP3
3. Tho√°t
Ch·ªçn (1/2/3): """).strip()
            
            if choice == '1':
                new_url = input("Nh·∫≠p URL m·ªõi: ").strip()
                if new_url:
                    music_list = downloader.parse_pixabay_page(new_url)
                    if music_list:
                        break
            elif choice == '2':
                return handle_direct_urls()
            elif choice == '3':
                print("üëã T·∫°m bi·ªát!")
                return
            else:
                print("‚ùå Vui l√≤ng ch·ªçn 1, 2 ho·∫∑c 3.")
        
        if not music_list:
            return
    
    # Hi·ªÉn th·ªã danh s√°ch
    downloader.display_music_list()
    
    # Nh·∫≠p range ƒë·ªÉ download
    try:
        print(f"\nüìù Nh·∫≠p range ƒë·ªÉ download (1-{len(music_list)}):")
        start = int(input("T·ª´ s·ªë: ").strip())
        end = int(input("ƒê·∫øn s·ªë: ").strip())
        
        # Nh·∫≠p th∆∞ m·ª•c l∆∞u (optional)
        folder_input = input("Th∆∞ m·ª•c l∆∞u (Enter = 'downloads'): ").strip()
        folder = folder_input if folder_input else "downloads"
        
        # T√πy ch·ªçn s·ªë threads
        print(f"\n‚öôÔ∏è  T√ôY CH·ªåN THREADING:")
        threads_input = input("S·ªë threads download (Enter = 4, t·ªëi ƒëa 8): ").strip()
        try:
            max_threads = int(threads_input) if threads_input else 4
            max_threads = min(max(max_threads, 1), 8)  # Gi·ªõi h·∫°n t·ª´ 1-8
        except ValueError:
            max_threads = 4
            print("‚ùå S·ªë kh√¥ng h·ª£p l·ªá, d√πng m·∫∑c ƒë·ªãnh 4 threads")
        
        # X√°c nh·∫≠n
        print(f"\nüîç S·∫º DOWNLOAD:")
        print(f"   - T·ª´ b√†i {start} ƒë·∫øn b√†i {end}")
        print(f"   - T·ªïng c·ªông: {end - start + 1} b√†i")
        print(f"   - Th∆∞ m·ª•c: {folder}")
        print(f"   - Threads: {max_threads}")
        
        confirm = input("\nX√°c nh·∫≠n download? (y/N): ").strip().lower()
        
        if confirm in ['y', 'yes']:
            downloader.download_music_range(start, end, folder, max_threads)
        else:
            print("‚ùå ƒê√£ h·ªßy download.")
            
    except KeyboardInterrupt:
        print("\n\n‚ùå ƒê√£ h·ªßy b·ªüi ng∆∞·ªùi d√πng.")
    except ValueError:
        print("‚ùå Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá.")
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")

if __name__ == "__main__":
    main()
